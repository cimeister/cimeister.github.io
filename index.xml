<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Clara Meister</title>
    <link>https://cimeister.github.io/</link>
      <atom:link href="https://cimeister.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Clara Meister</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Mon, 24 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://cimeister.github.io/media/icon_hu_982c5d63a71b2961.png</url>
      <title>Clara Meister</title>
      <link>https://cimeister.github.io/</link>
    </image>
    
    <item>
      <title>Example Talk</title>
      <link>https://cimeister.github.io/events/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/events/example/</guid>
      <description>


  
  
  
  
  





  
  
  














  
  
  
  


&lt;div class=&#34;callout flex px-4 py-3 mb-6 rounded-md border-l-4 bg-blue-100 dark:bg-blue-900 border-blue-500&#34; 
     data-callout=&#34;note&#34; 
     data-callout-metadata=&#34;&#34;&gt;
  &lt;span class=&#34;callout-icon pr-3 pt-1 text-blue-600 dark:text-blue-300&#34;&gt;
    &lt;svg height=&#34;24&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;m16.862 4.487l1.687-1.688a1.875 1.875 0 1 1 2.652 2.652L6.832 19.82a4.5 4.5 0 0 1-1.897 1.13l-2.685.8l.8-2.685a4.5 4.5 0 0 1 1.13-1.897zm0 0L19.5 7.125&#34;/&gt;&lt;/svg&gt;
  &lt;/span&gt;
  &lt;div class=&#34;callout-content dark:text-neutral-300&#34;&gt;
    &lt;div class=&#34;callout-title font-semibold mb-1&#34;&gt;Note&lt;/div&gt;
    &lt;div class=&#34;callout-body&#34;&gt;&lt;p&gt;Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.&lt;/p&gt;&lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Hugo Blox Builder&amp;rsquo;s 
 feature and link using the &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to this page bundle and link it using &lt;code&gt;links: [{ type: slides, url: path/to/file } ]&lt;/code&gt; in front matter&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using 
.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including 
 such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>UnigramLM: An Attempt at Writing The Missing Manual</title>
      <link>https://cimeister.github.io/blog/unigramlm/</link>
      <pubDate>Wed, 19 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/blog/unigramlm/</guid>
      <description>&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;:  This post is my attempt to write down the UnigramLM tokenization algorithm cleanly and explicitly because, well, I still haven&amp;rsquo;t found such a derivation and I think understanding the theory behind the method could help us make it better. I&amp;rsquo;ll formalize the generative model on which the algorithm&amp;rsquo;s assumptions [BETTER WORD/TERM] are based, derive the EM updates, explain why pruning is needed (and how it&amp;rsquo;s done), and point out the
spots where the practical implementation defined by the SentencePiece library diverges from the pretty mathematical models. Hopefully, this post points out some interesting potential extensions/revisions to the current implementation.&lt;/p&gt;
&lt;h3 id=&#34;intro-and-origins-of-this-blog-post&#34;&gt;Intro and origins of this blog post&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;(feel free to 
 this section)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;These days, tokenization is basically synonymous with Byte-pair Encoding (BPE). If you ask someone &amp;ldquo;do you know how tokenization works?&amp;rdquo;, there&amp;rsquo;s a decent chance you&amp;rsquo;ll get an answer like: &amp;ldquo;Yeah yeah, I know BPE.&amp;rdquo;  But tokenization != BPE. There are numerous (arguably better motivated) algorithms one could use for segmenting text into tokens. This post focuses on UnigramLM (the SentencePiece &amp;ldquo;unigram&amp;rdquo; model), which is a pretty far departure from the BPE approach&amp;hellip;&lt;/p&gt;
&lt;h4 id=&#34;why-look-at-unigramlm-now-and-not-just-make-bpe-better&#34;&gt;Why look at UnigramLM now (and not just &amp;ldquo;make BPE better&amp;rdquo;)?&lt;/h4&gt;
&lt;p&gt;Recent work keeps showing that tokenizers themselves can induce 
 and 
 across languages, dialects, and writing systems. A lot of the community response has (reasonably!) focused on patching BPE: adding constraints, regularizers, or parity-aware merges. Those are valuable, but there&amp;rsquo;s a risk in treating &amp;ldquo;tokenization = BPE + tweaks&amp;rdquo; as the whole design space. UnigramLM is a widely deployed alternative (T5, XLNet), and it comes from a fundamentally different modeling viewpoint. Instead of greedily merging pairs, it says: &amp;ldquo;let&amp;rsquo;s uncover latent tokens and treat tokenization like inference.&amp;rdquo; At least to me, that framing feels a lot more linguistically sane (or, at minimum, less like we&amp;rsquo;re playing subword Tetris). Taking that viewpoint seriously could open different and maybe cleaner directions for addressing tokenizer-induced unfairness&amp;mdash;not by iterating on one algorithm forever, but by re-examining the assumptions we bake into tokenization in the first place.&lt;/p&gt;
&lt;h4 id=&#34;why-this-blog-post&#34;&gt;Why this blog post&lt;/h4&gt;
&lt;p&gt;With the above motivation in mind, I figured I should actually understand the algorithm.
So I did what everyone does: I went to the 
. That&amp;hellip; didn&amp;rsquo;t get me very far. So then I went to the SentencePiece repo, hoping I could reconstruct the missing pieces from the code. After a brief flashback while staring at the C++ implementation to the terror of my undergraduate CS classes, I bailed on that approach too. Then I thought maybe the missing explanation was hiding in the HuggingFace documentation. But let&amp;rsquo;s just say that rabbit hole ended like this:&lt;/p&gt;



  
  &lt;blockquote class=&#34;border-l-4 border-neutral-300 dark:border-neutral-600 pl-4 italic text-neutral-600 dark:text-neutral-400 my-6&#34;&gt;
    &lt;p&gt;&lt;em&gt;The HuggingFace documentation&lt;/em&gt; [on UnigramLM] &lt;em&gt;describes a
tokeniser that doesn&amp;rsquo;t exist. It should not be relied on as an
explanation for UnigramLM, because it doesn&amp;rsquo;t even come close.&lt;/em&gt;&lt;br&gt;
–Claude&lt;/p&gt;

  &lt;/blockquote&gt;

&lt;p&gt;The original UnigramLM paper gives a nice high-level story, and the code clearly works in practice, but I couldn&amp;rsquo;t find a single place that actually spells out the full generative model, why the algorithm is mathematically sound, or how all the little &amp;ldquo;engineering details&amp;rdquo; (like pruning and vocabulary initialization) fit into that picture. This post is my attempt to provide an approachable but rigorous walkthrough of UnigramLM as a probabilistic model, showing why EM is a reasonable tool here, what the posterior over segmentations actually looks like, and how the SentencePiece-style implementation approximates/diverges from all of this in practice. If you&amp;rsquo;ve ever felt that UnigramLM is &amp;ldquo;clear enough to use, but not clear enough to explain on a whiteboard,&amp;rdquo; my hope is that this takes you the rest of the way to really understanding it, and maybe even extending it. Because at least I think its a pretty cool algorithm that deserves some of BPE&amp;rsquo;s limelight.&lt;/p&gt;
&lt;h2 id=&#34;sec:background&#34;&gt;Tokenization Background and Notation&lt;/h2&gt;
&lt;p&gt;So that we&amp;rsquo;re on the same page, let&amp;rsquo;s start with a formal definition of tokenization.&lt;/p&gt;
&lt;p&gt;Let ${\mathbf{{s}}}=\langle{s}_{1},{s}_2,\dots\rangle$ be a string&amp;mdash;a
sequence of characters (or bytes) such that ${s}_{t}\in{\Sigma}$ for a
base alphabet ${\Sigma}$. Let ${\mathcal{V}}$ be a finite set, where
each ${v}\in{\mathcal{V}}$ consists of a sequence of symbols from
${\Sigma}\cup{\Gamma}$, where ${\Gamma}$ denotes a finite set of
reserved symbols (e.g., whitespace markers, start/end tokens, etc.); we
refer to ${\mathcal{V}}$ as our &lt;strong&gt;vocabulary&lt;/strong&gt; and to ${v}$ as
&lt;strong&gt;pieces&lt;/strong&gt;.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; During tokenization, we wish to convert the sequence of
characters/bytes ${\mathbf{{s}}}$ into a sequence of tokens
${\mathbf{{v}}}=\langle {v}_{1},\dots,{v}_{{m}}\rangle$, each of which
is a piece in the set ${\mathcal{V}}$. We refer to this token sequence
as a &lt;strong&gt;segmentation&lt;/strong&gt; of ${\mathbf{{s}}}$, and it can informally be seen
as just a different way of representing the original string.&lt;/p&gt;
&lt;p&gt;A tokenization algorithm defines a mapping
${h}: {\Sigma}^* \rightarrow {\mathcal{V}}^*$ and the method for
learning the parameters of this mapping. The application of ${h}$ (which
we&amp;rsquo;ll call our &lt;strong&gt;tokenization function&lt;/strong&gt; here) to a string is sometimes
referred to as inference, although perhaps more commonly people just
call this process &amp;ldquo;tokenizing a string.&amp;rdquo; For example, the byte-pair
encoding (BPE) algorithm defines a ${h}$ that is parameterized by a list
of &lt;em&gt;merge&lt;/em&gt; pairs
$\boldsymbol{\mu}=\langle({v}_1, {v}_1&#39;),({v}_2, {v}_2&#39;), \dots \rangle$
and the algorithm for learning $\boldsymbol{\mu}$. At inference,
starting from the representation of ${\mathbf{{s}}}$ as just a sequence
of symbols from the base vocabulary ${\Sigma}$, ${h}_{\boldsymbol{\mu}}$
goes through the text $i=1, \dots |\boldsymbol{\mu}|$ times. At step
$i$, it replaces all co-occurrences of the pair $({v}_i, {v}_i&#39;)$ with a
new merged token (typically, of the form ${v}_i\circ{v}_i&#39;$).&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Importantly, we assume that ${\mathbf{{s}}}$ can be reconstructed from
${\mathbf{{v}}}$ via a &lt;strong&gt;detokenization function&lt;/strong&gt;
${g}: {\mathcal{V}}^* \rightarrow {\Sigma}^*$; often ${g}:$ is a simple
mapping like string concatenation with some special symbol handling,
e.g., ${g}({\mathbf{{v}}}) = {v}_{1}\circ\dots \circ {v}_{{m}}$. In what
follows, we consider ${g}$ fixed and treat it as part of the model
specification. All probabilities over strings and segmentations are
defined with respect to this fixed choice of ${g}$. Notably, given just
the vocabulary ${\mathcal{V}}$, there are often multiple valid
${\mathbf{{v}}}$ for which the application of our simple detokenization
function ${g}$ would lead to the same ${\mathbf{{s}}}$. In other words,
${g}$ is generally non-injective. We use
${\mathcal{T}}_{\mathcal{V}}({\mathbf{{s}}}) \mathrel{\stackrel{\textnormal{ def}}{=}}{g}^{-1}({\mathbf{{s}}}) = \{{\mathbf{{v}}}\in{\mathcal{V}}^* : {g}({\mathbf{{v}}}) = {\mathbf{{s}}}\}$
to refer to the set of all valid token sequences that produce
${\mathbf{{s}}}$, i.e., the set-valued inverse of ${g}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example 1&lt;/strong&gt; (A concrete example of the non-injectivity of $g$.).
&lt;em&gt;Consider a toy string ${\mathbf{{s}}}= hat$ and a small
vocabulary ${\mathcal{V}}= \{\text{h},\text{a},\text{t},\text{ha},\text{at}\}$.
Under our fixed detokenization function ${g}$ (simple concatenation of
token symbol sequences), the set of all valid segmentations of
${\mathbf{{s}}}$ is
$$\begin{aligned}
    {\mathcal{T}}_{\mathcal{V}}({\mathbf{{s}}})
    =\{
        \langle \text{h}, \text{a}, \text{t} \rangle,
        \langle \text{ha}, \text{t} \rangle,
        \langle \text{h}, \text{at} \rangle\}.
\end{aligned}
$$ where all three segmentations detokenize to the same
string ${\mathbf{{s}}}= \text{hat}$ under ${g}$.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;While it might not seem notable, the non-injectivity of ${g}$ is
actually an interesting property of most tokenization schemes. For one,
it&amp;rsquo;s motivated several variants of different tokenization algorithms in
which the inference rule&amp;mdash;the mapping
${h}:{\Sigma}^*\rightarrow{\mathcal{V}}^*$ that selects a particular
element of ${\mathcal{T}}_{\mathcal{V}}({\mathbf{{s}}})$&amp;mdash;is replaced
or redefined, for example by sampling from a posterior over
segmentations (Kudo 2018) or by changing the inference
objective to something like minimizing token sequence length
(Hofmann et. al., 2022; Schmidt et. al. 2024). It
It also means that we should distinguish between the &lt;strong&gt;canonical tokenization&lt;/strong&gt;
of ${\mathbf{{s}}}$, which is ${h}({\mathbf{{s}}})$, and any other valid segmentation
${\mathbf{{v}}}\in {\mathcal{T}}_{\mathcal{V}}({\mathbf{{s}}})$ with
${\mathbf{{v}}}\neq {h}({\mathbf{{s}}})$,
which are typically called &lt;strong&gt;non-canonical tokenizations&lt;/strong&gt;.
The existence of non-canonical
tokenizations has implications for how one should actually compute the
probability of a string under a language model using a given vocabulary.
See Cao and Rimell (2021) for a more detailed discussion of
non-canonical tokenizations and why they matter in practice.&lt;/p&gt;
&lt;h2 id=&#34;what-you-came-here-for-unigramlm&#34;&gt;What you came here for: UnigramLM&lt;/h2&gt;
&lt;p&gt;The UnigramLM tokenization algorithm (Kudo 2018) takes a
probabilistic-modeling approach to string tokenization. It defines an
${h}$, together with an algorithm for learning its parameters, by
treating tokenization as inference in a latent-variable generative model
over strings&amp;mdash;in particular, a unigram generative model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Few Sentence Description of UnigramLM&lt;/strong&gt;: UnigramLM is basically what it sounds like: a unigram language model. The only parameters of the tokenization scheme are a unigram probability distribution. When learning the tokenizer, we learn both the vocabulary and piece probabilities of this unigram model that (approximately) maximize corpus log-likelihood. At inference time, given a string, UnigramLM chooses the segmentation (sequence of pieces) that has the highest probability under this learned unigram model. In contrast to BPE’s greedy merge story, UnigramLM’s behavior is really &amp;ldquo;whichever segmentation makes the whole corpus most probable under this unigram model wins.&amp;rdquo;&lt;/p&gt;
&lt;h3 id=&#34;sec:gen_model&#34;&gt;Generative model&lt;/h3&gt;
&lt;p&gt;The UnigramLM tokenization algorithm assumes that each observed string
${\mathbf{{s}}}$ arises from a latent sequence of tokens
${\mathbf{{v}}}$, where tokens are drawn independently from a fixed
probability distribution, i.e., from a unigram distribution over a fixed
vocabulary. The data-generating distribution can thus be defined in
terms of the unigram probabilities
${\boldsymbol{\phi}}\in \Delta^{|{\mathcal{V}}| - 1}$. Before we get to
the definition of the data-generating distribution though, we have to
establish some other definitions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Warning about notation:&lt;/strong&gt; To reduce the number of nested subscripts
(and other similarly offensive notational choices), I&amp;rsquo;m going to
primarily use random variables to describe this problem. Don&amp;rsquo;t worry,
you&amp;rsquo;ll still get a nice sprinkling of nested subscripts even with the
random variables! Just fewer than without. Sorry... As is standard,
uppercase letters will denote random variables (e.g., $X$, $Z$), and
bold uppercase letters will denote sequences of them (e.g., $\mathbf X$,
$\mathbf Z$).&lt;/p&gt;
&lt;p&gt;Formally, let ${V}$ be our token-valued random variable: a categorical
random variable on ${\mathcal{V}}$ with
$\sum_{{v}\in{{\mathcal{V}}}}{P({V}={v};{\boldsymbol{\phi}})}=1$.
Occasionally for shorthand, we&amp;rsquo;ll use
${\phi_{v}}= {P({V}={v};{\boldsymbol{\phi}})}$ to refer to the unigram
probability of the piece ${v}$. Let ${\mathbf{V}}$ be a random variable
taking values in the space of token &lt;em&gt;sequences&lt;/em&gt;
${\mathbf{{v}}}\in {\mathcal{V}}^*$. For the distribution of
${\mathbf{V}}$ to be a valid probability distribution on
${\mathcal{V}}^*$, we must specify a length prior, i.e., a random
variable ${M}$ on $\mathbb{N}$ with
$\sum_{{m}=0}^\infty P({M}={m})=1$.&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; The UnigramLM algorithm then
assumes token sequence
${\mathbf{{v}}}=\langle{v}_1,\dots,{v}_{m}\rangle$ are generated as
&lt;/p&gt;
$$
    {m}\sim {M},\quad {v}_{t}\stackrel{\text{i.i.d.}}{\sim} {\small\mathrm{Categorical}}({\boldsymbol{\phi}}) (t=1,\dots,{m})
\tag{1}
$$&lt;p&gt;We can thus define the distribution of ${\mathbf{V}}$ as
&lt;/p&gt;
$$
    P({\mathbf{V}}={\mathbf{{v}}};{\boldsymbol{\phi}}) \mathrel{\stackrel{\textnormal{ def}}{=}} P({M}=|{\mathbf{{v}}}|)\prod_{t=1}^{|{\mathbf{{v}}}|}{P({V}={v}_{{t}};{\boldsymbol{\phi}})}
\tag{2}
$$&lt;p&gt;
The likelihood of a sequence conditional on a given length ${m}$ is then
simply the product of its piece probabilities, i.e.,
Eq. (2) where the length prior term cancels out:
&lt;/p&gt;
$$
   P({\mathbf{V}}={\mathbf{{v}}}\mid {M}={m};{\boldsymbol{\phi}}) = \prod_{{t}=1}^{{m}} {P({V}={v}_{t};{\boldsymbol{\phi}})},
\tag{3}
$$&lt;p&gt;
One thing to note is that the parameters of
${P({V};{\boldsymbol{\phi}})}{\cdot}$ are completely specified by
${\boldsymbol{\phi}}$. This isn&amp;rsquo;t the case with
$P({\mathbf{V}};{\boldsymbol{\phi}})$, for which the parameters of ${M}$
must also be known to fully specify the distribution. We won&amp;rsquo;t add any
additional notation to $P({\mathbf{V}};{\boldsymbol{\phi}})$ to specify
the parameters of ${M}$, though, since ${M}$ is pretty much always
ignored. Rather, in yet another moment of &amp;rsquo;engineering convenience&#39;
winning out over &amp;rsquo;theoretical elegance&amp;rsquo;, most people just compute token
sequence probabilities in UnigramLM using Eq. 3.&lt;/p&gt;
&lt;p&gt;Given the deterministic mapping ${g}$ from tokens to strings, we can
derive the distribution over strings&amp;mdash;our data-generating
distribution&amp;mdash;as a pushforward of the distribution over tokens. Let
${\mathbf{S}}$ be a random variable on ${\Sigma}^*$. The following
relationship holds: &lt;/p&gt;
$$
\begin{aligned}
    P({\mathbf{S}}={\mathbf{{s}}};{\boldsymbol{\phi}}) \mathrel{\stackrel{\textnormal{ def}}{=}}\sum_{{\mathbf{{v}}}\in {\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}})} P({\mathbf{V}}={\mathbf{{v}}};{\boldsymbol{\phi}})
\end{aligned}
\tag{4}
$$&lt;h4 id=&#34;some-useful-relationships-between--and-&#34;&gt;Some useful relationships between ${\mathbf{V}}$ and ${\mathbf{S}}$.&lt;/h4&gt;
&lt;p&gt;We can see from
Eq. (4) that distribution of ${\mathbf{S}}$ is
simply the marginal probability distribution over valid segmentations of
${\mathbf{{s}}}$ under ${\mathcal{V}}$. Applying Bayes&amp;rsquo; rule then gives
us the posterior over segmentations for a fixed ${\mathbf{{s}}}$:
&lt;/p&gt;
$$
P({\mathbf{V}}={\mathbf{{v}}}\mid {\mathbf{S}}={\mathbf{{s}}}; {\boldsymbol{\phi}}) = 
\begin{cases}
    &amp;\frac{P({\mathbf{V}}={\mathbf{{v}}};{\boldsymbol{\phi}})}{P({\mathbf{S}}={\mathbf{{s}}};{\boldsymbol{\phi}})} \quad \text{if } {\mathbf{{v}}}\in{\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}})\\\ &amp;0 \quad \quad \text{ otherwise.}
\end{cases}
\tag{5}
$$&lt;p&gt; By just moving some terms in
Eq. (5) around, we also get the definition of the
joint distribution over strings and token sequences:
&lt;/p&gt;
$$P({\mathbf{S}}={\mathbf{{s}}}, {\mathbf{V}}={\mathbf{{v}}};{\boldsymbol{\phi}}) = P({\mathbf{V}}={\mathbf{{v}}};{\boldsymbol{\phi}})\mathbb{1}\\{{\mathbf{{v}}}\in{\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}})\\},
$$&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;
&lt;p&gt;For the moment, let&amp;rsquo;s assume that we know ${\boldsymbol{\phi}}$, or at
least have estimates for these parameters. At inference time (i.e., when
segmenting text into tokens), the UnigramLM tokenization algorithm aims
to find the most likely segmentation of ${\mathbf{{s}}}$ into tokens
${\mathbf{{v}}}= \langle {v}_1, {v}_2, \dots\rangle$ under the
generative model (defined above) with these parameters. To this end, it
uses a Viterbi-style algorithm: &lt;/p&gt;
$$
\begin{aligned}
{{h}_{{\boldsymbol{\phi}}}({\mathbf{{s}}})}&amp;= \arg\max_{{\mathbf{{v}}}\in {\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}})} P({\mathbf{V}}={\mathbf{{v}}}\mid {\mathbf{S}}={\mathbf{{s}}}; {\boldsymbol{\phi}})\\\ 
&amp;= \arg\max_{{\mathbf{{v}}}\in {\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}})} P({\mathbf{V}}={\mathbf{{v}}};{\boldsymbol{\phi}})\\\ 
&amp;= \arg\max_{{\mathbf{{v}}}\in {\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}})} P({M}=|{\mathbf{{v}}}|)\prod_{t=1}^{|{\mathbf{{v}}}|}{P({V}={v}_{t};{\boldsymbol{\phi}})}\\\ 
&amp;\overset{?}{=} \arg\max_{{\mathbf{{v}}}\in {\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}})} \prod_{t=1}^{|{\mathbf{{v}}}|}{P({V}={v}_{t};{\boldsymbol{\phi}})}
\end{aligned}
\tag{6}
$$&lt;p&gt; where
the second line follows from the relationship in
Eq. (5)
($P({\mathbf{S}}={\mathbf{{s}}};{\boldsymbol{\phi}})$ does not depend on
${\mathbf{{v}}}$ and so it doesn&amp;rsquo;t affect the argmax). As we can see in
Eq. 6, the length prior (${M}$) is part of the
posterior distribution and should thus affect the Viterbi segmentation;
intuitively speaking, it biases the distribution towards token sequences
of certain lengths.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example 2&lt;/strong&gt; (Effect of the length prior on Viterbi segmentation).
&lt;em&gt;Suppose a string ${\mathbf{{s}}}$ admits two valid segmentations
${\mathbf{{v}}}^{(1)}$ and ${\mathbf{{v}}}^{(2)}$ under ${\mathcal{V}}$,
with lengths $|{\mathbf{{v}}}^{(1)}| = 1$ and
$|{\mathbf{{v}}}^{(2)}| = 3$. Assume that the unigram probabilities are
such that
$$
\prod_{{t}=1}^{|{\mathbf{{v}}}^{(1)}|} {P({V}={v}^{(1)}_{t};{\boldsymbol{\phi}})}
    =
    \prod_{{t}=1}^{|{\mathbf{{v}}}^{(2)}|} {P({V}={v}^{(2)}_{t};{\boldsymbol{\phi}})}
$$
so the two segmentations tie if we ignore the length prior. Now let the
length prior favor shorter sequences, e.g. $$
P({M}=1) = 0.9,
    \qquad
    P({M}=3) = 0.1
$$ Then the full sequence probabilities become
$$
\begin{aligned}
    P({\mathbf{V}}={\mathbf{{v}}}^{(1)};{\boldsymbol{\phi}})
    &amp;= P({M}=1) \prod_{{t}=1}^{|{\mathbf{{v}}}^{(1)}|} {P({V}={v}^{(1)}_{t};{\boldsymbol{\phi}})}
     = 0.9 \cdot C,\\\ 
    P({\mathbf{V}}={\mathbf{{v}}}^{(2)};{\boldsymbol{\phi}})
    &amp;= P({M}=3) \prod_{{t}=1}^{|{\mathbf{{v}}}^{(2)}|} {P({V}={v}^{(2)}_{t};{\boldsymbol{\phi}})}
     = 0.1 \cdot C,
\end{aligned}
$$ for some common factor $C$. The Viterbi segmentation
under the full model (including the length prior) is therefore
${\mathbf{{v}}}^{(1)}$, while under the approximation that drops
$P({M}=\cdot)$, the two segmentations are equally probable. This
illustrates that the length prior can in principle have a non-trivial
affect on the inference result.&lt;/em&gt;
:::&lt;/p&gt;
&lt;p&gt;SentencePiece (and all other implementations of UnigramLM) effectively
assume the length prior term is flat (i.e., sequence length
probabilities are assumed to be constant for all valid lengths) and
drops it, so in practice UnigramLM inference is usually done with the
length-free objective (last line in Eq. 6). Unless otherwise specified, when
talking about inference, we will assume use of
this objective for faithfulness to the original
algorithm. I&amp;rsquo;m spelling this out because it&amp;rsquo;s a silent approximation and
it could potentially be interesting to look into the effects of this
design choice!&lt;/p&gt;
&lt;p&gt;The true parameters of the generative process ${\boldsymbol{\phi}}$ are
unknown, however; this includes both the piece probabilities
${\phi_{v}}$ and the underlying vocabulary ${\mathcal{V}}$ over which
they are defined. The UnigramLM tokenization algorithm (described next)
proposes a method for coming up with an estimate of these parameters
from text data.&lt;/p&gt;
&lt;h3 id=&#34;learning-model-parameters&#34;&gt;Learning Model Parameters&lt;/h3&gt;
&lt;p&gt;Maximum likelihood estimation (MLE)&amp;mdash;a standard approach to estimating
model parameters&amp;mdash;aims to find the model parameters that maximize the
log-likelihood of our data. Under the UnigramLM assumptions about the
generative process of strings, our &amp;ldquo;complete&amp;rdquo; dataset actually consists
of $({\mathbf{{s}}},{\mathbf{{v}}})$ pairs, i.e., strings and the
sequence of tokens that produced them. Thus, our complete dataset looks
like $\mathcal{X} = \{({\mathbf{{s}}}_i,{\mathbf{{v}}}_i)\}_{i=1}^K$ and
the complete-data log likelihood is defined as: &lt;/p&gt;
$$
\begin{aligned}
    {\mathcal{L}}(\mathcal{X}; {\boldsymbol{\phi}}) &amp;\mathrel{\stackrel{\textnormal{ def}}{=}}\log\prod_{i=1}^KP({\mathbf{S}}={\mathbf{{s}}}_i, {\mathbf{V}}={\mathbf{{v}}}_i;{\boldsymbol{\phi}})\\\ 
    &amp;= \sum_{i=1}^K\log P({\mathbf{S}}={\mathbf{{s}}}_i, {\mathbf{V}}={\mathbf{{v}}}_i;{\boldsymbol{\phi}})
\end{aligned}
\tag{7}
$$&lt;p&gt;
Eq. (7) is typically referred to as the
&lt;em&gt;complete&lt;/em&gt; data log-likelihood. If we actually had this complete data
(and we knew ${\mathcal{V}}$), we would simply find the
${\boldsymbol{\phi}}$ that maximizes
Eq. (7), which would be a fairly clean
problem that is easy to solve given our assumptions about the underlying
distributions. However, we only see the &amp;ldquo;post-processed&amp;rdquo; strings
${\mathbf{{s}}}= {g}({\mathbf{{v}}})$; the exact underlying pieces that
form that string are unknown (can be any in
${\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}})$ and we don&amp;rsquo;t even know
${\mathcal{V}}$!). So, we can instead try to maximize our &lt;em&gt;observed&lt;/em&gt;
data log-likelihood, i.e., the likelihood of just our strings under our
data-generating distribution defined in
Eq. (4). Given our &amp;ldquo;useful&amp;rdquo; relationships in
from earlier, we can define this likelihood in terms of
${\boldsymbol{\phi}}$: &lt;/p&gt;
$$
\begin{aligned}
    {\mathcal{L}}({\mathcal{C}}; {\boldsymbol{\phi}}) &amp;\mathrel{\stackrel{\textnormal{ def}}{=}}\log\prod_{i=1}^KP({\mathbf{S}}={\mathbf{{s}}}_i;{\boldsymbol{\phi}})\\\ 
    &amp;= \log\prod_{i=1}^K\sum_{{\mathbf{{v}}}\in {\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}}_i)} P({\mathbf{V}}={\mathbf{{v}}};{\boldsymbol{\phi}})
    %&amp; \text{\color{gray}{Sub def in \cref{eq:pushforward}}}
    \\\ 
    &amp;= \sum_{i=1}^K \log\sum_{{\mathbf{{v}}}\in{\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}}_i)} P({\mathbf{V}}={\mathbf{{v}}};{\boldsymbol{\phi}})
   % \\\ &amp;= \sum_{i=1}^K \log\sum_{\tokens\in\allsegmentations_{\vocab}(\str_i)} \qprior{|\tokens|}\prod_\tokindex \unigramdist{\token_\tokindex}
\end{aligned}
\tag{8}
$$&lt;p&gt; where
${\mathcal{C}}= \{{\mathbf{{s}}}\mid {\mathbf{{s}}}, \_ \in \mathcal{X} \}$
is simply our observed set of strings, i.e., our corpus. Unfortunately,
Eq. (8) is a difficult quantity to maximize
directly due to the log&amp;ndash;sum structure. Luckily, the
expectation-maximization (EM) algorithm provides us a route for working
with this situation.&lt;/p&gt;
&lt;h3 id=&#34;sec:unigram_em&#34;&gt;The Expectation-Maximization Algorithm in the Context of UnigramLM&lt;/h3&gt;
&lt;p&gt;EM was designed for exactly the use case where wish to get MLE estimates
for a data-generating process in which only part of the data is
unobserved.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TL;DR of the application of the EM algorithm to UnigramLM&lt;/strong&gt;: EM is an
iterative algorithm for approximating MLE estimates. The E step computes
the expected complete data log-likelihood under current beliefs about
model parameters (in our case, ${{\boldsymbol{\phi}}^{(n)}}$); this
quantity is standing in for observed data log-likelihood, which is a
much more difficult quantity to compute. The M step then solves for the
free parameters (in our case, ${\boldsymbol{\phi}}$) that maximize this
quantity, and then updates our current beliefs to the new quantity.&lt;/p&gt;
&lt;p&gt;In more detail now: the EM algorithm uses Jensen&amp;rsquo;s inequality to relate
the &lt;em&gt;expected value&lt;/em&gt; of the complete data log-likelihood to the
&lt;em&gt;observed&lt;/em&gt; data log-likelihood, i.e., relating the expected value of
Eq. (7) to
Eq. (8). This is exactly the connection made
by Kudo (2018) (even if not explicitly) when introducing their
algorithm for approximating the parameters ${\boldsymbol{\phi}}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Expected complete-data log-likelihood under observed data and current parameters.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let ${{\boldsymbol{\phi}}^{(n)}}$ denote our current belief about what
the unigram parameters might be (more discussion on how we can
initialize this distribution coming up!). For now, we will assume that
the vocabulary is fixed. These random variables adhere to our original
definitions in 
{reference-type=&amp;ldquo;ref+label&amp;rdquo;
reference=&amp;ldquo;sec:gen_model&amp;rdquo;}. Note that when we use simply
${\boldsymbol{\phi}}$, we are referring to the distributions (and
corresponding random variables) induced by a generic
${\boldsymbol{\phi}}$; these are the entities for which our parameters
are free variables that we are optimizing.&lt;/p&gt;
&lt;p&gt;The expected complete data log-likelihood under
${{\boldsymbol{\phi}}^{(n)}}$&amp;mdash;which we denote as
${\mathcal{Q}}({\boldsymbol{\phi}};{{\boldsymbol{\phi}}^{(n)}})$&amp;mdash;follows
simply from taking the expectated value of
Eq. (7), given our observed data
${\mathcal{C}}$ and our current model parameters
${{\boldsymbol{\phi}}^{(n)}}$, i.e., the expected value under the
posterior ${\mathbf{V}}\mid {\mathbf{S}};{{\boldsymbol{\phi}}^{(n)}}$.
&lt;/p&gt;
$$
\begin{aligned}
{\mathcal{Q}}({\boldsymbol{\phi}};{{\boldsymbol{\phi}}^{(n)}})
&amp;\mathrel{\stackrel{\textnormal{ def}}{=}} \mathop{\mathrm{\mathbb{E}}}
\big[{\mathcal{L}}(\mathcal{X}; {\boldsymbol{\phi}}) \mid {\mathcal{C}}, {{\boldsymbol{\phi}}^{(n)}}\big]\\\ 
&amp;= \underset{ {\mathbf{V}}\mid {\mathbf{S}};{{\boldsymbol{\phi}}^{(n)}}}{\mathop{\mathrm{\mathbb{E}}}}\big[\sum_{i=1}^K \log P({\mathbf{S}}, {\mathbf{V}};{\boldsymbol{\phi}})\mid {\mathcal{C}}\big]\\\ 
    &amp;= \sum_{i=1}^K\underset{{\mathbf{{v}}}\sim{\mathbf{V}}\mid {\mathbf{S}}={\mathbf{{s}}}_i;{{\boldsymbol{\phi}}^{(n)}}}{\mathop{\mathrm{\mathbb{E}}}}\big[\log P({\mathbf{S}}={\mathbf{{s}}}_i, {\mathbf{V}}={\mathbf{{v}}};{\boldsymbol{\phi}})\big]
\end{aligned}
$$&lt;p&gt; In words, we can think of
${\mathcal{Q}}({\boldsymbol{\phi}};{{\boldsymbol{\phi}}^{(n)}})$ as the
expected complete data log-likelihood where the (latent) segmentations
are induced by the posterior with parameters
${{\boldsymbol{\phi}}^{(n)}}$, while the log-likelihood inside is
evaluated using the candidate parameters ${\boldsymbol{\phi}}$.&lt;/p&gt;
&lt;p&gt;Now we will show how this quantity relates to the observed data
log-likelihood.&lt;/p&gt;
&lt;h4 id=&#34;observed-data-log-likelihood-and-jensens-inequality&#34;&gt;Observed data log-likelihood and Jensen&amp;rsquo;s inequality.&lt;/h4&gt;
&lt;p&gt;We start with a reminder of Jensen&amp;rsquo;s inequality, applied to our
definition of $P({\mathbf{S}}={\mathbf{{s}}};{\boldsymbol{\phi}})$. For
any valid distribution probability $P({\mathbf{V}}={\mathbf{{v}}})$,
Jensen&amp;rsquo;s inequality tells us &lt;/p&gt;
$$
\begin{aligned}
\log P({\mathbf{S}}={\mathbf{{s}}};{\boldsymbol{\phi}})
&amp;= \log \sum_{{\mathbf{{v}}}\in {\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}})} P({\mathbf{V}}={\mathbf{{v}}})\,\frac{P({\mathbf{V}}={\mathbf{{v}}};{\boldsymbol{\phi}})}{P({\mathbf{V}}={\mathbf{{v}}})}\\\ 
&amp;\ge \sum_{{\mathbf{{v}}}\in {\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}})} P({\mathbf{V}}={\mathbf{{v}}})\,\log \frac{P({\mathbf{V}}={\mathbf{{v}}};{\boldsymbol{\phi}})}{P({\mathbf{V}}={\mathbf{{v}}})}
\end{aligned}
$$&lt;p&gt; If we choose $P({\mathbf{V}}={\mathbf{{v}}})$ to be
$P({\mathbf{V}}={\mathbf{{v}}}\mid {\mathbf{S}}= {\mathbf{{s}}};{{\boldsymbol{\phi}}^{(n)}})$&amp;mdash;the
posterior under our current parameter beliefs for a fixed
${\mathbf{{s}}}$&amp;mdash;and apply this to our definition of the observed data
log-likelihood from
Eq. (8), we get &lt;/p&gt;
$$
\begin{aligned}
{\mathcal{L}}&amp;({\mathcal{C}};{\boldsymbol{\phi}})= \sum_{i=1}^K \log\sum_{{\mathbf{{v}}}\in{\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}}_i)} P({\mathbf{V}}={\mathbf{{v}}};{\boldsymbol{\phi}})\\\ 
&amp;\ge \sum_{i=1}^K\sum_{{\mathbf{{v}}}\in {\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}}_i)}P({\mathbf{V}}={\mathbf{{v}}}\mid {\mathbf{S}}={\mathbf{{s}}}_i;{{\boldsymbol{\phi}}^{(n)}})
\big[\log P({\mathbf{V}}={\mathbf{{v}}};{\boldsymbol{\phi}})-\log P({\mathbf{V}}={\mathbf{{v}}}\mid {\mathbf{S}}={\mathbf{{s}}}_i;{{\boldsymbol{\phi}}^{(n)}})\big]\\\ 
&amp;= \sum_{i=1}^K\sum_{{\mathbf{{v}}}\in {\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}}_i)}P({\mathbf{V}}={\mathbf{{v}}}\mid {\mathbf{S}}={\mathbf{{s}}}_i;{{\boldsymbol{\phi}}^{(n)}})
\log P({\mathbf{S}}={\mathbf{{s}}}_i, {\mathbf{V}}={\mathbf{{v}}};{\boldsymbol{\phi}})\nonumber\\\ 
&amp; \qquad\qquad\qquad\qquad-\sum_{i=1}^K\sum_{{\mathbf{{v}}}\in {\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}}_i)}P({\mathbf{V}}={\mathbf{{v}}}\mid {\mathbf{S}}={\mathbf{{s}}}_i;{{\boldsymbol{\phi}}^{(n)}})\log P({\mathbf{V}}={\mathbf{{v}}}\mid {\mathbf{S}}={\mathbf{{s}}}_i;{{\boldsymbol{\phi}}^{(n)}})\\\ 
&amp;= \underbrace{\sum_{i=1}^K\underset{{\mathbf{{v}}}\sim {\mathbf{V}}\,\mid\, {\mathbf{S}}={\mathbf{{s}}}_i;{{\boldsymbol{\phi}}^{(n)}}}{\mathop{\mathrm{\mathbb{E}}}}\big[\log P({\mathbf{S}}={\mathbf{{s}}}_i, {\mathbf{V}}={\mathbf{{v}}};{\boldsymbol{\phi}})\big]}_{{\mathcal{Q}}({\boldsymbol{\phi}};{{\boldsymbol{\phi}}^{(n)}})} + \sum_{i=1}^K\underbrace{\underset{{\mathbf{{v}}}\sim {\mathbf{V}}\,\mid\, {\mathbf{S}}={\mathbf{{s}}}_i;{{\boldsymbol{\phi}}^{(n)}}}{\mathop{\mathrm{\mathbb{E}}}}\big[\log P({\mathbf{V}}={\mathbf{{v}}}\mid {\mathbf{S}}={\mathbf{{s}}}_i;{{\boldsymbol{\phi}}^{(n)}})\big]}_{{\mathrm{H}}\big({\mathbf{V}}\,\mid \,{\mathbf{S}}={\mathbf{{s}}}_i;{{\boldsymbol{\phi}}^{(n)}}\big)}
\\\ 
&amp;\geq {\mathcal{Q}}({\boldsymbol{\phi}};{{\boldsymbol{\phi}}^{(n)}})
\end{aligned}
\tag{9}
$$&lt;p&gt; Note that when going from the second to third lines in
Eq. (9), we make use of the fact that for any
${\mathbf{{v}}}\in {\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}}_i)$ we
have
$P({\mathbf{S}}={\mathbf{{s}}}_i, {\mathbf{V}}={\mathbf{{v}}};{\boldsymbol{\phi}}) = P({\mathbf{V}}={\mathbf{{v}}};{\boldsymbol{\phi}})$
by definition. Then, we&amp;rsquo;re simply using the equivalence of these
values with the definitions of expected values and (Shannon) entropy,
respectively.&lt;/p&gt;
&lt;p&gt;Eq. (9) is typically referred to as the evidence lower
bound (ELBO)&amp;mdash;a proxy objective that is often used in machine learning.
For example, it&amp;rsquo;s used for training variational autoencoders, where it
provides a tractable lower bound on the intractable log-likelihood of
the data under a latent-variable model. In the case of EM, we go one
step further and use one of the components of the ELBO as our proxy
objective for observed data log-likelihood: the expected complete data
log-likelihood
${\mathcal{Q}}({\boldsymbol{\phi}};{{\boldsymbol{\phi}}^{(n)}})$. And
this is the basis the EM algorithm, which iteratively updates
${\boldsymbol{\phi}}$ by choosing the value of it that maximizes
${\mathcal{Q}}({\boldsymbol{\phi}};{{\boldsymbol{\phi}}^{(n)}})$ until
convergence. I&amp;rsquo;ll omit the proof of why EM should converge (for a fixed
${\mathcal{V}}$) since, well, it&amp;rsquo;s in a lot of ML textbooks (you know,
those ones we all swear we&amp;rsquo;ll read cover-to-cover someday...). But
after all those derivations, I do think it&amp;rsquo;s helpful to look at our
ideal and actual objectives side-by-side, just to see what the
difference is:&lt;/p&gt;
$$
\underbrace{\sum_{i=1}^K \log\sum_{{\mathbf{{v}}}\in{\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}}_i)} P({\mathbf{V}}={\mathbf{{v}}};{\boldsymbol{\phi}})}_{\text{objective we&#39;d ideally be maximizing }} \qquad\qquad \underbrace{\sum_{i=1}^K\sum_{{\mathbf{{v}}}\in {\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}}_i)}P({\mathbf{V}}={\mathbf{{v}}}\mid {\mathbf{S}}={\mathbf{{s}}}_i;{{\boldsymbol{\phi}}^{(n)}})
\log P({\mathbf{V}}={\mathbf{{v}}};{\boldsymbol{\phi}})}_{\text{objective we maximize iteratively with EM}}
$$&lt;h3 id=&#34;the-unigramlm-algorithm&#34;&gt;The UnigramLM Algorithm&lt;/h3&gt;
&lt;p&gt;The UnigramLM algorithm is typically seen as a &amp;ldquo;simple&amp;rdquo; application of
EM. This, however, is not exactly the case. Importantly, EM assumes that
the support of the distribution whose parameters we&amp;rsquo;re trying to
estimate is known (and fixed), i.e., that we know ${\mathcal{V}}$. But,
as discussed earlier, we don&amp;rsquo;t know ${\mathcal{V}}$! The UnigramLM
algorithm addresses this by beginning with an intentionally overcomplete
initial vocabulary and progressively reducing it through a heuristic
pruning step, which is done &lt;em&gt;after&lt;/em&gt; an iteration of the standard E-step
and M-step, throughout which ${\mathcal{V}}$ is held fixed. In short, as
the algorithm iteratively re-estimates the model parameters, it
gradually shrinks ${\mathcal{V}}$ toward the desired final size by
removing pieces that are seemingly unimportant for achieving good corpus
log-likelihood. You can think of this as putting your vocabulary on a
strict likelihood-based diet: pieces that don&amp;rsquo;t contribute enough to
explaining the data get gently but firmly removed.&lt;/p&gt;
&lt;p&gt;Where its necessary, to make this dependence explicit, we will use
${{\mathcal{V}}_{{n}}}$ to denote the current vocabulary. To reduce
notational clutter, in defining the below algorithm, we&amp;rsquo;ll use just
${\mathcal{V}}$; at step $n$ of the algorithm, you can assume
${\mathcal{V}}={{\mathcal{V}}_{{n}}}$ (and that all random variables
are defined over ${{\mathcal{V}}_{{n}}}$) unless otherwise stated.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;d like to look at a trimmed down version of the pseudocode, you can 
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Initialization:&lt;/strong&gt; Define an initial vocabulary ${\mathcal{V}}_0$.
This could be something like all possible substrings of
${\mathcal{C}}$, subject to a maximum length constraint.&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;
Initialize ${\boldsymbol{\phi}}^{(0)}$ by some heuristic: the
simplest would be uniform initialization, i.e., all pieces are
assigned probability $1/|{\mathcal{V}}_0|$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Perform EM for $n=1, \dots N$ iterations or until piece
probability estimates converge:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;i.  &lt;strong&gt;E-step&lt;/strong&gt; (Expected data log-likelihood computation): The
E-step in EM is for computing the expected complete data
log-likelihood under our current parameter beliefs
${\mathcal{Q}}({\boldsymbol{\phi}};{{\boldsymbol{\phi}}^{(n)}})$.
It turns out that expected token counts are a sufficient
statistic for the M-step objective in this case, and so our
problem boils down to computing expected token counts under
${{\boldsymbol{\phi}}^{(n)}}$. To see why this is the case&amp;hellip; First, we define the count function on token sequences as
&lt;/p&gt;
$$c_{{v}}({\mathbf{{v}}}) \mathrel{\stackrel{\textnormal{ def}}{=}}\sum_{t=1}^{|{\mathbf{{v}}}|}\mathbb{1}\\{{v}_{t}= {v}\\} \tag{10}$$&lt;p&gt;
Then, note that for any valid ${\mathbf{{s}}},{\mathbf{{v}}}$
such that
${\mathbf{{v}}}\in{\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}})$,
we can write &lt;/p&gt;
$$
\begin{aligned}
        \log P({\mathbf{S}}={\mathbf{{s}}}, {\mathbf{V}}={\mathbf{{v}}};{\boldsymbol{\phi}})&amp;=\log P({\mathbf{V}}={\mathbf{{v}}};{\boldsymbol{\phi}})\\\ 
        &amp;=\log P({M}=|{\mathbf{{v}}}|)+\sum_{t=1}^{|{\mathbf{{v}}}|}\log {P({V}={v}_{t};{\boldsymbol{\phi}})}\\\ 
        &amp;=\log P({M}=|{\mathbf{{v}}}|)+ \sum_{{v}\in{\mathcal{V}}} c_{{v}}({\mathbf{{v}}})\log {P({V}={v};{\boldsymbol{\phi}})}
        \end{aligned}
$$&lt;p&gt; Substituting these relationships into our definition of
${\mathcal{Q}}({\boldsymbol{\phi}};{{\boldsymbol{\phi}}^{(n)}})$
and using the linearity of
expectations rule, we get
&lt;/p&gt;
$$\begin{aligned}
          {\mathcal{Q}}({\boldsymbol{\phi}};{{\boldsymbol{\phi}}^{(n)}})
          = \underbrace{\sum_{i=1}^K\underset{{\mathbf{{v}}}\sim {\mathbf{V}}\mid {\mathbf{S}}={\mathbf{{s}}}_i;{{\boldsymbol{\phi}}^{(n)}}}{\mathop{\mathrm{\mathbb{E}}}}[\log P({M}=|{\mathbf{{v}}}|)]}_{\text{constant in }{\boldsymbol{\phi}}}
          +\sum_{i=1}^K\sum_{{v}\in{\mathcal{V}}}
          \underbrace{\underset{{\mathbf{{v}}}\sim {\mathbf{V}}\mid {\mathbf{S}}={\mathbf{{s}}}_i;{{\boldsymbol{\phi}}^{(n)}}}{\mathop{\mathrm{\mathbb{E}}}\left[ c_{{v}}({\mathbf{{v}}})\right]}}_{\mathrel{\stackrel{\textnormal{ def}}{=}}\widetilde{c}_{{v}}({\mathbf{{s}}}_i;{{\boldsymbol{\phi}}^{(n)}})}\log {P({V}={v};{\boldsymbol{\phi}})}
          \end{aligned}
  \tag{11}
  $$&lt;p&gt; where
$\widetilde{c}_{{v}}({\mathbf{{s}}};{{\boldsymbol{\phi}}^{(n)}})$
are simply expected token counts under unigram model parameters
${{\boldsymbol{\phi}}^{(n)}}$, which can be computed as
$\widetilde{c}_{{v}}({\mathbf{{s}}};{{\boldsymbol{\phi}}^{(n)}})= \sum_{{\mathbf{{v}}}\in{\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}})} c_{{v}}({\mathbf{{v}}}) P({\mathbf{V}}={\mathbf{{v}}}\mid {\mathbf{S}}={\mathbf{{s}}};{{\boldsymbol{\phi}}^{(n)}})$. Lastly, if we define the corpus-level expected counts as
&lt;/p&gt;
$$
          \widehat{c}_{{v}}({\mathcal{C}};{\boldsymbol{\phi}}) \mathrel{\stackrel{\textnormal{ def}}{=}} \sum_{{\mathbf{{s}}}\in{\mathcal{C}}} \widetilde{c}_{{v}}({\mathbf{{s}}};{\boldsymbol{\phi}})
  \tag{12}
  $$&lt;p&gt; and substitute them into our expansion in
Eq. 11, then the equality reduces to
&lt;/p&gt;
$$
          {\mathcal{Q}}({\boldsymbol{\phi}};{{\boldsymbol{\phi}}^{(n)}})
          = \text{const} + \underbrace{\sum_{{v}\in{\mathcal{V}}}\widehat{c}_{{v}}({\mathcal{C}};{{\boldsymbol{\phi}}^{(n)}})\log {P({V}={v};{\boldsymbol{\phi}})}}_{\mathrel{\stackrel{\textnormal{ def}}{=}}\bar{{\mathcal{Q}}}({\boldsymbol{\phi}};{{\boldsymbol{\phi}}^{(n)}})}
  \tag{13}
  $$&lt;p&gt;
where we have added the definition of $\bar{{\mathcal{Q}}}$
(simply ${\mathcal{Q}}$ without the &amp;ldquo;$\mathrm{const}$&amp;rdquo; term)
since it will be useful later. From the above, we can see that
the posterior expected counts are sufficient statistics for the
M-step objective
${\mathcal{Q}}({\boldsymbol{\phi}};{{\boldsymbol{\phi}}^{(n)}})$.
In practice, the per-string expected counts
$\widetilde{c}_{{v}}({\mathbf{{s}}};{\boldsymbol{\phi}})$ can
be computed efficiently using a forward&amp;ndash;backward dynamic
program defined over the segmentation lattice induced by
${\mathcal{T}}_{{\mathcal{V}}}({\mathbf{{s}}})$. In words, this
lattice forms a directed acyclic graph: nodes correspond to
positions in the string and edges originating from the nodes
correspond to tokens ${v}\in
          {\mathcal{V}}$ that can begin at that position and end at
another (i.e., pieces whose symbol sequences match the
substring). Each edge is weighted by the token&amp;rsquo;s probability
under the current parameters, ${\phi^{(n)}_{v}}$. Valid paths
in this graph correspond to a valid segmentation of
${\mathbf{{s}}}$. The forward&amp;ndash;backward algorithm then
marginalizes over all valid paths in this graph to compute the
posterior probability of each token&amp;rsquo;s occurrence, from which the
expected counts follow.
A somewhat interesting observation is that this method of
getting counts uses an inference procedure that is different
from what is done when actually tokenizing text. In the latter
case, only the maximum probability segmentation is ultimately
used. Here, though, we consider all
segmentations of a ${\mathbf{{s}}}$ that have non-zero
probability, weighting the token counts from this segmentation
(token sequence) by the probability of the segmentation under
our current parameters ${{\boldsymbol{\phi}}^{(n)}}$. Also of
note is that this is where a length prior &lt;em&gt;could&lt;/em&gt; have an effect
on the model parameters we learn. But the term is often never
actually used in the model definition.&lt;/p&gt;
&lt;p&gt;ii.  &lt;strong&gt;M-step&lt;/strong&gt; (maximize ${\boldsymbol{\phi}}$ and update
${{\boldsymbol{\phi}}^{(n)}}$): In the M-step, we want to
maximize
${\mathcal{Q}}({\boldsymbol{\phi}};{{\boldsymbol{\phi}}^{(n)}})$
with respect to ${\boldsymbol{\phi}}$ subject to these
parameters giving us a valid probability distribution, i.e.,
$\sum_{{v}\in{\mathcal{V}}}{\phi_{v}}=1$ and
${\phi_{v}}\ge 0$. Subbing in the relationship established in
Eq. 13, this actually boils down to a
relatively simple problem: finding the ${\boldsymbol{\phi}}$
that maximizes the probability of having observed the expected
counts that we got from the segmenting the corpus according to
our prior model parameter beliefs: &lt;/p&gt;
$$
\begin{aligned}
        \max_{{\boldsymbol{\phi}}}&amp;\sum_{{v}\in{{\mathcal{V}}}}\widehat{c}_{{v}}({\mathcal{C}};{{\boldsymbol{\phi}}^{(n)}})\log {P({V}={v};{\boldsymbol{\phi}})}\\\ 
        &amp;\text{s.t.}\quad
        \sum_{{v}\in{\mathcal{V}}}{\phi_{v}}=1,{\phi_{v}}\ge 0
        \end{aligned}
$$&lt;p&gt; The solution (normalized expected counts) is
very recognizable, as it is essentially the same as the MLE for
a standard multinomial distribution, albeit using expected
counts rather than pure counts: &lt;/p&gt;
$$
        {\phi^{(n+1)}_{v}}
        =
        \frac{\widehat{c}_{{v}}({\mathcal{C}};{{\boldsymbol{\phi}}^{(n)}})}
        {\sum_{{v}&#39;\in{{\mathcal{V}}}}\widehat{c}_{{v}&#39;}({\mathcal{C}};{{\boldsymbol{\phi}}^{(n)}})}.
\tag{14}
$$&lt;p&gt;
The length-prior term is constant in ${\boldsymbol{\phi}}$ and
does not alter the update (for fixed ${\mathcal{V}}$).&lt;/p&gt;
&lt;p&gt;iii.  &lt;strong&gt;Pruning:&lt;/strong&gt; After applying the above steps, the vocabulary
itself will not have changed (only the per-piece probabilities
are updated). Because the initial vocabulary ${\mathcal{V}}_0$
is typically over-complete (often
$|{\mathcal{V}}_0| \gg |{\mathcal{V}}|$), we want to trim it
down. UnigramLM achieves this by applying a pruning step
&lt;em&gt;within&lt;/em&gt; the EM iterations. Explicitly, at step $n$, it removes
$k_n$ of the least &amp;ldquo;important&amp;rdquo; pieces, leading to a new
${\mathcal{V}}_{n+1}$. Following pruning, the remaining
probabilities in ${\boldsymbol{\phi}}^{(n+1)}$ are renormalized
to form a valid distribution over ${\mathcal{V}}_{n+1}$. This
pruning is done until the vocabulary reaches the desired size.
Formally, let
$\bar{{\mathcal{Q}}}({\boldsymbol{\phi}}^{(n+1)};{{\boldsymbol{\phi}}^{(n)}})$
be our expected complete data log-likelihood under updated model
parameters (albeit still under the segmentations according to
${{\boldsymbol{\phi}}^{(n)}}$). The algorithm removes tokens
whose absence leads to the smallest decrease in (our proxy for)
observed data log-likelihood. Intuitively, we prune tokens that
contribute least to explaining the data under the current model.
We define the contribution (or &amp;ldquo;loss&amp;rdquo;) associated with token
${v}$ as the change (typically a decrease) in the corpus
log-likelihood when ${v}$ is removed from the model:
&lt;/p&gt;
$${L}({v})
            \mathrel{\stackrel{\textnormal{ def}}{=}}
            \bar{{\mathcal{Q}}}({\boldsymbol{\phi}}^{(n+1)};{{\boldsymbol{\phi}}^{(n)}}) 
            -
            \bar{{\mathcal{Q}}}({\boldsymbol{\phi}}^{(n+1)}_{-{v}};{{\boldsymbol{\phi}}^{(n)}_{-{v}}}),
\tag{15}
$$&lt;p&gt;
The notation ${{\boldsymbol{\phi}}^{(n)}_{-{v}}}$ in
Eq. (15) refers to the unigram distribution
obtained from ${{\boldsymbol{\phi}}^{(n)}}$ by removing ${v}$
from its support and renormalizing the remaining probabilities.
The corresponding string-level distribution is thus identical to
the one induced by ${{\boldsymbol{\phi}}^{(n)}}$, except that
all segmentations containing ${v}$ are assigned zero probability
and individual piece probabilities are renormalized over
${\mathcal{V}}\setminus \{{v}\}$ (this logic also applies to
${\boldsymbol{\phi}}^{(n+1)}_{-{v}}$). After computing
${L}({v})$ for all ${v}\in {\mathcal{V}}_n$, we remove the $k_n$
tokens with the smallest losses, where $k_n$ is a hyperparameter
chosen such that after some number of iterations, we ultimately
reach our desired vocabulary size.&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; Intuitively, this can be
seen as removing the tokens whose removal incurs the &lt;em&gt;least&lt;/em&gt;
penalty on the corpus log-likelihood. Notably, computing
$\bar{{\mathcal{Q}}}({\boldsymbol{\phi}}^{(n+1)}_{-{v}};{{\boldsymbol{\phi}}^{(n)}_{-{v}}})$
in Eq. (15) is very computationally expensive
since it requires a separate forward&amp;ndash;backward pass over the
corpus. We discuss some approximations to ${L}({v})$ in the
following section.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;approximations-of-&#34;&gt;Approximations of ${L}$.&lt;/h4&gt;
&lt;p&gt;Computing
$\bar{{\mathcal{Q}}}({\boldsymbol{\phi}}^{(n+1)}_{-{v}};{{\boldsymbol{\phi}}^{(n)}_{-{v}}})$
in 
{reference-type=&amp;ldquo;ref+label&amp;rdquo;
reference=&amp;ldquo;eq:tokenloss&amp;rdquo;} for a given ${v}$ generally requires a
separate forward&amp;ndash;backward pass over the corpus. This is because
disallowing the use of ${v}$ in segmentations changes both the set of
valid paths and the total probability of those paths.&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; The new
per-string marginal probabilities (and expected token counts) under
${\boldsymbol{\phi}}^{(n+1)}_{-{v}}$. cannot, in general, be recovered
from forward/backward marginals computed under
${{\boldsymbol{\phi}}^{(n)}}$. Hence, we would need a fresh
forward&amp;ndash;backward evaluation on the pruned lattice to obtain the exact
$\bar{{\mathcal{Q}}}({\boldsymbol{\phi}}^{(n+1)}_{-{v}};{{\boldsymbol{\phi}}^{(n)}_{-{v}}})$.&lt;/p&gt;
&lt;p&gt;Performing a separate forward&amp;ndash;backward pass for each piece in the
vocabulary whenever we want to prune is impractical for vocabularies of
any reasonable size. For example, if our initial vocabulary is a mere
100$k$, then computing per-piece losses would require 100$k$ forward
passes of the corpus on its own. In practice, approximations that reuse
the statistics computed from the current EM iteration are done. We
discuss those next. To avoid the need to resegment the corpus to compute
each ${v}$&amp;rsquo;s loss, several approximations can be used to compute
per-piece losses. A simple approximation would be to use as a token&amp;rsquo;s
loss its contribution to corpus log-likelihood, i.e.,
$\widehat{L}({v}) \approx
\widehat{c}_{{v}}({\mathcal{C}};{\boldsymbol{\phi}}^{(n+1)})\log {P({V}={v};{{\boldsymbol{\phi}}^{(n)}})}$.
An arguably more sound approximation (and the one used by the original
implementation of UnigramLM found in the SentencePiece library) is to
look at the change in corpus log-likelihood when simply replacing ${v}$
by the best alternative segmentation of that piece, i.e., the best
alternative segmentation of the string ${g}({v})$ when ${v}$ is not in
the vocabulary.&lt;/p&gt;
&lt;p&gt;Formally, let
${\mathbf{{v}}}&#39; = {h}_{{{\boldsymbol{\phi}}^{(n)}_{-{v}}}}({g}({v}))$
be the best segmentation of the string ${\mathbf{{s}}}= {g}({v})$ under
${{\boldsymbol{\phi}}^{(n)}_{-{v}}}$.&lt;sup id=&#34;fnref1:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; The approximate loss is then
the change to corpus log-likelihood when replacing every use of
${P({V}={v};{{\boldsymbol{\phi}}^{(n)}})}$ with
$\prod_{{t}=1}^{|{\mathbf{{v}}}&#39;|}{P({V}={v}&#39;_{t}; {{\boldsymbol{\phi}}^{(n)}_{-{v}}})}$
under the new renormalized unigram probabilities
${{\boldsymbol{\phi}}^{(n)}_{-{v}}}$. This loss can be computed
concisely as:
&lt;/p&gt;
$$
\widehat{L}({v})\approx \widehat{c}_{{v}}({\mathcal{C}};{{\boldsymbol{\phi}}^{(n)}})\left[\log {P({V}={v};{{\boldsymbol{\phi}}^{(n)}})} - \log \prod_{{t}=1}^{|{\mathbf{{v}}}&#39;|}{P({V}={v}&#39;_{t}; {{\boldsymbol{\phi}}^{(n)}_{-{v}}})}\right]
$$&lt;p&gt;&lt;strong&gt;Example 3&lt;/strong&gt; (Toy pruning example). &lt;em&gt;Suppose our corpus contains the
string ${\mathbf{{s}}}= \text{``internationalization&#39;&#39;}$&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;and our vocabulary includes the tokens $$
\{
    \text{international},\quad
    \text{inter},\quad
    \text{national},\quad
    \text{ization},\quad
    \text{al},\ldots
\}
$$ Assume that under the current parameters
${{\boldsymbol{\phi}}^{(n)}}$, the posterior expected corpus-level
counts are
$$
\widehat{c}_{\text{international}}({\mathcal{C}};{{\boldsymbol{\phi}}^{(n)}})
    \ll
    \widehat{c}_{\text{inter}}({\mathcal{C}};{{\boldsymbol{\phi}}^{(n)}}),
    \widehat{c}_{\text{national}}({\mathcal{C}};{{\boldsymbol{\phi}}^{(n)}}),
    \widehat{c}_{\text{ization}}({\mathcal{C}};{{\boldsymbol{\phi}}^{(n)}})
$$&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;To approximate the contribution of ${v}_{\text{international}}$, we
consider its best alternative segmentation when it is removed from the
vocabulary. Let
$$
{\mathbf{{v}}}&#39; = \langle \text{inter}, \text{national} \rangle
$$ be
the Viterbi segmentation of the string ${g}(\text{international})$ under
the renormalized distribution ${{\boldsymbol{\phi}}^{(n)}_{-{v}}}$. The
approximate loss associated with pruning $\text{international}$ is then
$$
\begin{aligned}
    \widehat{L}(\text{international};{{\boldsymbol{\phi}}^{(n)}})
    &amp;\approx
    \widehat{c}_{\text{international}}({\mathcal{C}};{{\boldsymbol{\phi}}^{(n)}})
    \log \frac{
        {P({V}=\text{international};{{\boldsymbol{\phi}}^{(n)}})}
    }{
        {P({V}=\text{inter}; {{\boldsymbol{\phi}}^{(n)}_{-{v}}})}
        \cdot
        {P({V}=\text{national}; {{\boldsymbol{\phi}}^{(n)}_{-{v}}})}
    }.
\end{aligned}
$$ Intuitively, if ${v}_{\text{international}}$ is both
rare (small
$\widehat{c}_{\text{international}}({\mathcal{C}};{{\boldsymbol{\phi}}^{(n)}})$)
and easily replaced by a segmentation whose product of probabilities is
similar to ${P({V}=\text{international};{{\boldsymbol{\phi}}^{(n)}})}$,
then its (approximate) loss will be small, making it a good candidate
for pruning.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;While this approximation does not account for changes in other valid
paths&amp;rsquo; probabilities that might happen as a result of removing ${v}$
from the vocabulary, it seems to work fairly well in practice as a
pruning heuristic (although I don&amp;rsquo;t believe that anyone has actually
tried to run the algorithm with the real, brute-force loss computation).&lt;/p&gt;
&lt;h4 id=&#34;sec:pseudocode&#34;&gt;Concise Pseudocode&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;Algorithm UnigramLM-Train(C, V_target_size, V0, phi0, k_n):
    V   &amp;lt;- V0
    phi &amp;lt;- phi0

    while |V| &amp;gt; V_target_size:

        # ---- E-step ----
        hat_c[v] &amp;lt;- 0 for all v in V
        for each string s in C:
            lattice &amp;lt;- build_lattice(s, V)
            tilde_c &amp;lt;- forward_backward_expected_counts(lattice, phi)
            for each v in V:
                hat_c[v] &amp;lt;- hat_c[v] + tilde_c[v]

        # ---- M-step ----
        Z &amp;lt;- sum_{v in V} hat_c[v]
        for each v in V:
            phi[v] &amp;lt;- hat_c[v] / Z

        # ---- Pruning (approx loss) ----
        for each v in V:
            v_alt &amp;lt;- viterbi_best_segmentation(g(v), V \ {v}, phi)
            alt_prob &amp;lt;- product_{t in v_alt} phi[t]
            Lhat[v] &amp;lt;- hat_c[v] * ( log(phi[v]) - log(alt_prob) )

        V &amp;lt;- V \ bottom_k_tokens_by(Lhat, k_n)
        phi &amp;lt;- renormalize(phi over V)

    return V, phi
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;implementation-in-the-sentencepiece-library&#34;&gt;Implementation in the SentencePiece library&lt;/h2&gt;
&lt;p&gt;In practice, the UnigramLM algorithm as we know it is largely defined by
the public SentencePiece implementation, since @kudo-2018-subword give
only a high-level description and leave many engineering choices
under-specified. The library makes a number of concrete design decisions
that go beyond the abstract EM + pruning picture above.&lt;/p&gt;
&lt;h2 id=&#34;text-preprocessing&#34;&gt;Text Preprocessing.&lt;/h2&gt;
&lt;p&gt;Arguably some of the more critical design choices to be aware of are
those pertaining to normalization and pretokenization, as these change
which segmentations are feasible. SentencePiece advertises that it does
not apply any pretokenization, but I think that depends on your
definition of pretokenization... By default the library, collapses
whitespace, inserts a dummy-prefix marker, and treats whitespace (and
script/number boundaries) as explicit segmentation cues, i.e., as
markers that can be suffixes or prefixes of pieces, but that pieces
cannot cross. Most of these behaviors can be disabled via training flags
but the fact that they&amp;rsquo;re used is not well advertised. It also applies
NFKC normalization by default.&lt;/p&gt;
&lt;h4 id=&#34;initialization&#34;&gt;Initialization.&lt;/h4&gt;
&lt;p&gt;The seed vocabulary is not &amp;ldquo;all substrings up to length $L$&amp;rdquo;:
SentencePiece uses a version of the Enhanced (Extended) Suffix Array
procedure to mine a large lexicon of frequent substrings from the corpus
(on the order of $10^6$ pieces by default), subject to length and
frequency thresholds.&lt;/p&gt;
&lt;h4 id=&#34;em-updates&#34;&gt;EM Updates.&lt;/h4&gt;
&lt;p&gt;SentencePiece runs a fixed EM+prune schedule rather than iterating EM to
convergence on a fixed vocabulary. Each outer iteration consists of a
small fixed number of EM &amp;ldquo;sub-iterations&amp;rdquo; (typically two), after which
the vocabulary is pruned by a fixed shrinking factor, and training stops
once the target vocabulary size is reached. SentencePiece does not use
the plain MLE M-step update from
Eq. (14). Instead, it adopts a Variational Bayesian
approach with a Dirichlet prior, replacing expected counts with their

:
${\phi^{(n+1)}_{v}}\propto\exp(\Psi(\widehat{c}_{{v}}({\mathcal{C}};{{\boldsymbol{\phi}}^{(n)}})+\alpha_{v}))$.
While it might not seem like a large change to the original update rule,
this choice is implicitly adding a prior belief about the the number of
counts we should observe for each token. Explicitly, we&amp;rsquo;re now
calculating the geometric mean of a posterior Dirichlet distribution,
where we&amp;rsquo;re added in the belief token ${v}$ will be observed
$\alpha_{{v}}$ times. Notably, SentencePiece uses an improper Haldane
prior ($\alpha_{v}= 0$ for all ${v}\in{\mathcal{V}}$). This choice
essentially has the opposite effect of performing standard additive
smoothing: it&amp;rsquo;s always the case that $\exp(\psi(x)) &lt; x$, however, for
small $x$ (rare tokens), the relative &amp;ldquo;discount&amp;rdquo; is significantly
larger. It thus acts as a regularizer that disproportionately penalizes
tokens with low expected counts, sending their assigned probability mass
closer to zero. This is done on top of a 
 for tokens whose expected counts are below a certain threshold.&lt;/p&gt;
&lt;h4 id=&#34;pruning&#34;&gt;Pruning.&lt;/h4&gt;
&lt;p&gt;Pruning is performed as described above in the approximations section,
i.e., a piece&amp;rsquo;s loss is approximated by assuming that the removed
piece&amp;rsquo;s probability mass transfers to its best alternative Viterbi
segmentation
(${{h}_{{\boldsymbol{\phi}}}({\mathbf{{s}}})}_{{\mathcal{V}}_{{n}}}({v})$).
Notably, pieces whose expected counts are below a fixed value (0.5) are
pre-pruned. Also, not all pruning is done within the EM iterations;
there is a final pruning step that removes tokens with the lowest
estimated probabilities in order to get to the final desired vocabulary
size.&lt;/p&gt;
&lt;p&gt;Taken together, these implementation details instantiate one particular,
very specific version of the abstract UnigramLM model described above,
albeit the one that people are typically referring to (rather than an
implementation-free mathematical ideal) when talking about &amp;ldquo;the
UnigramLM tokenization algorithm.&amp;rdquo;&lt;/p&gt;
&lt;h4 id=&#34;acknowledgments&#34;&gt;&lt;em&gt;Acknowledgments&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;As with pretty much any technical work I&amp;rsquo;ve written, Tiago Pimentel provided critical commentary and recommendations for this blogpost. Thanks, PhD sibling :)&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;${v}$ are also sometimes called subwords; we avoid this naming
because ${v}$ need not align with orthographic words, in their
typical definition.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;$\circ$ denotes string concatenation and when applied to tokens,
indicates the pieces&amp;rsquo; symbols are concatenated together (perhaps
with some special formatting if symbols from ${\Gamma}$ are present
in the piece).&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;The distribution can also be made proper with the use of an EOS
symbol, which is the more common way of specifying a language model.
The use of ${M}$ in this situation (a non-autoregressive model) is a
bit more general (if the distribution of ${M}$ follows a power law,
then our distribution over token sequences could equivalently be
represented using an EOS symbol). The use of ${M}$ though allows us
to handle sequence length without adding a special token to our
vocabulary.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;There are several ways that this seed vocabulary can be created. The Enhanced Suffix Array is one of the more common algorithms. Often, pretokenization is performed on the corpus and one of the more common pretokenization rules splits on whitespace, preventing pieces from crossing whitespace boundaries, although that&amp;rsquo;s kind of an arbitrary rule&amp;hellip;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;Explicitly, relative to the original model, two coupled changes
occur when removing ${v}$ from ${{\mathcal{V}}_{{n}}}$: (i) the
feasible set of paths shrinks from
${\mathcal{T}}_{{{\mathcal{V}}_{{n}}}}({\mathbf{{s}}})$ to
${\mathcal{T}}_{{\mathcal{V}}_{n+1}}({\mathbf{{s}}})$ (all
segmentations using ${v}$ are removed); (ii) the per-edge weights
change after the renormalization of ${{\boldsymbol{\phi}}^{(n)}}$
and the marginal probabilities of remaining paths must be
recomputed.&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;This segmentation may need to include an UNK token depending on
the base vocabulary.&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Pandas</title>
      <link>https://cimeister.github.io/projects/pandas/</link>
      <pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/projects/pandas/</guid>
      <description>&lt;p&gt;Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PyTorch</title>
      <link>https://cimeister.github.io/projects/pytorch/</link>
      <pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/projects/pytorch/</guid>
      <description>&lt;p&gt;PyTorch is a Python package that provides tensor computation (like NumPy) with strong GPU acceleration.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>scikit-learn</title>
      <link>https://cimeister.github.io/projects/scikit/</link>
      <pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/projects/scikit/</guid>
      <description>&lt;p&gt;scikit-learn is a Python module for machine learning built on top of SciPy and is distributed under the 3-Clause BSD license.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>A Formal Perspective on Byte-Pair Encoding</title>
      <link>https://cimeister.github.io/publications/zouharal-acl-findings-23/</link>
      <pubDate>Sat, 01 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/zouharal-acl-findings-23/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Measure-theoretic Characterzation of Tight Language Model</title>
      <link>https://cimeister.github.io/publications/dual-acl-23-a/</link>
      <pubDate>Sat, 01 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/dual-acl-23-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On the Efficacy of Sampling Adapters</title>
      <link>https://cimeister.github.io/publications/meisteral-acl-23/</link>
      <pubDate>Sat, 01 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/meisteral-acl-23/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tokenization and the Noiseless Channel</title>
      <link>https://cimeister.github.io/publications/zouharal-acl-23/</link>
      <pubDate>Sat, 01 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/zouharal-acl-23/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On the Usefulness of Embeddings, Clusters and Strings for Text Generation Evaluation</title>
      <link>https://cimeister.github.io/publications/pimentelal-iclr-23/</link>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/pimentelal-iclr-23/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mutual Information and Hallucinations in Abstractive Summarization</title>
      <link>https://cimeister.github.io/publications/vanderpoelal-emnlp-22/</link>
      <pubDate>Thu, 01 Dec 2022 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/vanderpoelal-emnlp-22/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Analyzing Wrap-Up Effects through an Information-Theoretic Lens</title>
      <link>https://cimeister.github.io/publications/meisteral-acl-22-b/</link>
      <pubDate>Sun, 01 May 2022 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/meisteral-acl-22-b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Estimating the Entropy of Linguistic Distributions</title>
      <link>https://cimeister.github.io/publications/aroraal-acl-22/</link>
      <pubDate>Sun, 01 May 2022 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/aroraal-acl-22/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On the probability–quality paradox in language generation generation</title>
      <link>https://cimeister.github.io/publications/meisteral-acl-22-a/</link>
      <pubDate>Sun, 01 May 2022 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/meisteral-acl-22-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cluster-based Evaluation of Automatically Generated Text</title>
      <link>https://cimeister.github.io/publications/pimentelal-arxiv-22/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/pimentelal-arxiv-22/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Locally Typical Sampling</title>
      <link>https://cimeister.github.io/publications/meisteral-pre-22/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/meisteral-pre-22/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Naturalistic Causal Probing for Morpho-Syntax</title>
      <link>https://cimeister.github.io/publications/aminial-tacl-22/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/aminial-tacl-22/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On Decoding Strategies for Neural Text Generators</title>
      <link>https://cimeister.github.io/publications/wiheral-tacl-22/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/wiheral-tacl-22/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Conditional Poisson Stochastic Beams</title>
      <link>https://cimeister.github.io/publications/meisteral-emnlp-2021-b/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/meisteral-emnlp-2021-b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Keyword2Text: A Plug-and-Play Method for Controlled Text Generation</title>
      <link>https://cimeister.github.io/publications/pascualal-emnlp-2021/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/pascualal-emnlp-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On Homophony and Rényi Entropy</title>
      <link>https://cimeister.github.io/publications/pimentelal-emnlp-2021-b/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/pimentelal-emnlp-2021-b/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Phone-level Uniform Information Density across and within Languages</title>
      <link>https://cimeister.github.io/publications/pimentelal-emnlp-2021-a/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/pimentelal-emnlp-2021-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Revisiting the Uniform Information Density Hypothesis</title>
      <link>https://cimeister.github.io/publications/meisteral-emnlp-2021-a/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/meisteral-emnlp-2021-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Determinantal Beam Search</title>
      <link>https://cimeister.github.io/publications/meisteral2-acl-21/</link>
      <pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/meisteral2-acl-21/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Is Sparse Attention more Interpretable?</title>
      <link>https://cimeister.github.io/publications/meisteral3-acl-21/</link>
      <pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/meisteral3-acl-21/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Language Model Evaluation Beyond Perplexity</title>
      <link>https://cimeister.github.io/publications/meisteral-acl-21/</link>
      <pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/meisteral-acl-21/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Cognitive Regularizer for Language Modeling</title>
      <link>https://cimeister.github.io/publications/weial-acl-21/</link>
      <pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/weial-acl-21/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Testing Machine Translation via Referential Transparency</title>
      <link>https://cimeister.github.io/publications/heal-icse-2021/</link>
      <pubDate>Sun, 23 May 2021 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/heal-icse-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Searching for Search Errors in Neural Morphological Inflection</title>
      <link>https://cimeister.github.io/publications/forster-eacl-21/</link>
      <pubDate>Mon, 19 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/forster-eacl-21/</guid>
      <description></description>
    </item>
    
    <item>
      <title>If Beam Search is the Answer, What was the Question?</title>
      <link>https://cimeister.github.io/publications/meisteral-emnlp-20/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/meisteral-emnlp-20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Best-First Beam Search</title>
      <link>https://cimeister.github.io/publications/meisteral-tacl-20/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/meisteral-tacl-20/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Machine Translation Testing via Pathological Invariance</title>
      <link>https://cimeister.github.io/publications/guptaal-fse-2020/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/guptaal-fse-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Structure-Invariant Testing for Machine Translation</title>
      <link>https://cimeister.github.io/publications/heal-icse-2020/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/heal-icse-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Generalized Entropy Regularization or: There&#39;s Nothing Special about Label Smoothing</title>
      <link>https://cimeister.github.io/publications/meisteral-acl-20/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/publications/meisteral-acl-20/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
