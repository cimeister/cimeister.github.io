<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Recent &amp; Upcoming Talks | Clara Meister</title>
    <link>https://cimeister.github.io/event/</link>
      <atom:link href="https://cimeister.github.io/event/index.xml" rel="self" type="application/rss+xml" />
    <description>Recent &amp; Upcoming Talks</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 02 Jan 2017 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://cimeister.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Recent &amp; Upcoming Talks</title>
      <link>https://cimeister.github.io/event/</link>
    </image>
    
    <item>
      <title>Evidence for the uniform information density hypothesis in modern NLP models</title>
      <link>https://cimeister.github.io/talk/evidence-for-the-uniform-information-density-hypothesis-in-modern-nlp-models/</link>
      <pubDate>Mon, 02 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/talk/evidence-for-the-uniform-information-density-hypothesis-in-modern-nlp-models/</guid>
      <description>&lt;table class=&#34;table&#34;&gt;
  &lt;head&gt;
    &lt;base target=&#34;_blank&#34;&gt;
  &lt;/head&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th scope=&#34;col&#34; style=&#39;white-space:nowrap&#39;&gt;Location&lt;/th&gt;
      &lt;th scope=&#34;col&#34; style=&#39;white-space:nowrap&#39;&gt;Date&amp;emsp;&amp;emsp;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Berkeley&#39;s NLP Seminar&lt;/td&gt;
      &lt;td&gt;14.07.21&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MIT&#39;s Computational Psycholinguistics Lab&lt;/td&gt;
      &lt;td&gt;16.06.21&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;In this talk, I will review two recent works that have operationalized the uniform information density (UID) hypothesis for use in models of natural language processing. In machine translation, it has been frequently observed that texts assigned high probability (i.e., low surprisal) are not necessarily what humans perceive to be high quality language. Alternatively, text decoded using beam search, a popular heuristic decoding method, often scores well in terms of both qualitative and automatic evaluation metrics, such as BLEU. We show that beam search can be framed as a UID-enforcing decoding objective and that there exists a strong relationship between BLEU and the extent to which UID is adhered to in natural language text.
In a follow up work, we explore the effects of directly incorporating an operationalization of UID into a language model&amp;rsquo;s training objective. Specifically, we augment the canonical MLE objective with a regularizer that encodes UID. In experiments on ten languages spanning five language families, we find that using UID regularization consistently improves perplexity in language models, having a larger effect when training data is limited. Moreover, via an analysis of generated sequences, we find that UID-regularized language models have other desirable properties, e.g., they generate text that is more lexically diverse.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>If beam search is the answer, what was the question?</title>
      <link>https://cimeister.github.io/talk/if-beam-search-is-the-answer-what-was-the-question/</link>
      <pubDate>Mon, 02 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://cimeister.github.io/talk/if-beam-search-is-the-answer-what-was-the-question/</guid>
      <description>&lt;table class=&#34;table&#34;&gt;
  &lt;head&gt;
    &lt;base target=&#34;_blank&#34;&gt;
  &lt;/head&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th scope=&#34;col&#34; style=&#39;white-space:nowrap&#39;&gt;Location&lt;/th&gt;
      &lt;th scope=&#34;col&#34; style=&#39;white-space:nowrap&#39;&gt;Date&amp;emsp;&amp;emsp;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;University of Amsterdam&#39;s computational linguistics seminar&lt;/td&gt;
      &lt;td&gt;23.03.21&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NLP with Friends&lt;/td&gt;
      &lt;td&gt;03.02.21&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DeepMind&#39;s machine translation reading group&lt;/td&gt;
      &lt;td&gt;11.12.20&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural language generators frequently leads to low-quality results. Rather, most state-of-the-art results on language generation tasks are attained using beam search despite its overwhelmingly high search error rate. This implies that the MAP objective alone does not express the properties we desire in text, which merits the question&amp;hellip; if beam search is the answer, what was the question? We frame beam search as the exact solution to a different decoding objective in order to gain insights into why high probability under a model alone may not indicate adequacy. We find that beam search enforces uniform information density in text, a property motivated by cognitive science. We suggest a set of decoding objectives that explicitly enforce this property and find that exact decoding with these objectives alleviates the problems encountered when decoding poorly calibrated language generation models. Additionally, we analyze the text produced using various decoding strategies and see that, in our neural machine translation experiments, the extent to which this property is adhered to strongly correlates with BLEU.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
