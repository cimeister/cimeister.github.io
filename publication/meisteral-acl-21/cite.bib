@inproceedings{meister+al.acl21,
 title = {All of Linguistics in a Single Statistic? Looking Beyond Perplexity},
 author = {Meister, Clara and 
Cotterell, Ryan},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
 month = {August},
 year = {2021},
 address = {Online},
 publisher = {Association for Computational Linguistics},
 abstract = {We propose a new approach to exploring the linguistic properties learned by language models: we ask how well they learn the statistical tendencies of natural language. To answer this question, we analyze whether text generated from language models reflects the statistical tendencies present in the human-generated text on which they were trained.
We provide a framework---paired with significance tests---for evaluating the fit of language models to both theoretical and empirical linguistic distributions.
We find that neural language models appear to learn only a subset of linguistic distributions, but align much more closely with empirical trends than theoretical laws (when present). Further, the fit to different distributions is incredibly dependent on both model architecture and generation strategy. As concrete examples, text generated under the nucleus sampling scheme adheres more closely to the type--token relationship of natural language than text produced using standard ancestral sampling; text from LSTMs reflects the natural language distributions over length, stopword, and symbols suprisingly well.},
}

